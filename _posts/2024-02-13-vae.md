---
layout: post
title:  VAE
---

<div class="message">
  Variational Auto Encoders, Discriminative modeling, Generative modeling
</div>

**Generative modeling** is a type of unsupervised learning to create a model that learns the probability distribution of the training data given to it.
It can be used for :
1. Density estimation - Learning the distribution allows us to identify outliers, and hence handle unpredictable behaviour.
2. Sample generation - Learning the underlying distribution of the training data allows us to uncover biases in the data and hence create better datasets.

Generative modeling is finding p(x). Conditional generative modeling is p(x|y).
Latent variable models are a type of generative models. It includes AEs, VAEs and GANs.


**Auto Encoder (AE)** is used for compressing information in a latent layer or recreating an 
original image from corrupted image.
<p align="center">
    <img src="public/post_images/autoencoder.png" alt="Auto encoder application" width="250px">
</p>
An encoder maps the input $$x$$ to a low dimensional latent space $$z$$. This is an unsupervised problem =>
we do not have labels associated with the training images. The decoder tries to reconstruct the original image $$x'$$.
The objective is to minimize the distance between $$x$$ and $$x'$$.    
\begin{equation} 
L(x,x') = ||x-x'||^2 
\label{eq:loss_ae} 
\end{equation}   
The latent layer introduces a probability distribution on $$z$$.    
<p align="center">
    <img src="public/post_images/blog-AE.png" alt="Auto encoder">
</p>
This equation along with the low dimensional nature of $$z$$ introduces an information bottleneck, 
which tries to compress as much information as possible about $$x$$ into $$z$$.    
This is deterministic. As long as the green, yellow and pink boxes remain same (no 
change to the NN weights), we will get same $$x'$$ for the same input $$x$$. Meaning we can
reproduce the changes every time.    

**Variational Auto Encoder (VAE)**    
VAEs make this yellow box ($$z$$) stochastic. For each variable in $$z$$ you learn an associated
&mu; and &sigma;. This is what allows us to get *new* images, by sampling from the distribution of 
a $$z(\mu, \sigma, \epsilon)$$.    
<p align="center">
    <img src="public/post_images/blog-VAE.png" alt="Variational auto encoder">
</p>
Now the encoder tries to learn the probability distribution of $$z$$ given $$x$$,
and the decoder tries to learn a new probability distribution of $$x'$$ given $$z$$ i.e.   
Encoder computes $$ q_\Phi(z|x) $$    
Decoder computes $$ p_\theta(x|z) $$    
$$ \Phi $$ and $$ \theta $$ are the weights of the NNs. So the loss function for VAEs is   
\begin{equation} 
L(\Phi, \theta, x) = (\text{reconstruction loss}) + (\text{regularization term})
\label{eq:loss_vae} 
\end{equation}
This reconstruction loss is same as AE's \eqref{eq:loss_ae}, while the regularization loss 
places a prior on $$z$$ so as to try to enforce all the z's we learn to follow this prior $$p(z)$$. 
This way we learn z $$q_\Phi(z|x)$$ (Encoder) as close to the prior $$p(z)$$ as possible i.e. we minimize   
\begin{equation} 
D(\underbrace{q_\phi(z|x)}_{\text{learned z}} || \underbrace{p(z)}_{\text{fixed prior on z}})
\label{eq:loss_vae_regularization} 
\end{equation}
This prevents the NN from overfitting.

***Why regularize?***
We want the following properties in the latent space:
1. Continuity - closer points in $$z$$ should create similar images in $$x'$$
2. Completeness - sampling from $$z$$ must be a meaningful $$x'$$

<div class="row">
  <div class="column">
    <img src="public/post_images/vae_not_regularized_z.png" alt="Non regularized latent space" style="width:100%">
  </div>
  <div class="column">
    <img src="public/post_images/vae_regularized_z.png" alt="Regularized latent space" style="width:100%">
  </div>
</div>

Equation \eqref{eq:loss_vae_regularization} is actually the KL divergence between the 
two distributions.   

>How to select a prior? Gaussian FTW!

We know that the expectation of a random variable is $$ \mathbb{E} [f(x)]=\int xf(x)dx $$   
By chain rule, we have $$ P(x,y)=P(x|y)P(y) $$   
From Bayes theorem, we have $$ P(x|y)=\frac{P(y|x)P(x)}{P(y)} $$   
KL divergence is given by $$ D_{KL} (P||Q)=\int p(x) log(\frac{p(x)}{q(x)})dx $$   

***Reparametrization trick***
<div class="message">
Why can't we backpropogate with a stochastic variable? 
</div>
Sampling introduces randomness.   
=> it is no longer a function of parameters $$f(\mu, \sigma)$$
![Reparametrization trick](public/post_images/vae_reparametrization.jpg)

### References:
- Bayesian reasoning vs Frequentist [understanding](https://wiki.santafe.edu/images/2/2e/Bayesian-Reasoning-for-Intelligent-People-DeDeo.pdf)
- MIT lecture on Deep Generative Modeling [6.S191](https://www.youtube.com/watch?v=3G5hWM6jqPk&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=4)
- Stanford lecture on VAE [CS 229](https://www.youtube.com/watch?v=-TPFg-RG-KY)
- VAE math [video](https://www.youtube.com/watch?v=iwEzwTTalbg)